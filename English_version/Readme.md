# Readme
#### This is our team repositoryÔºÅ
Team member:<br>
PB22111627 Yin Yiming<br>
PB22111649 He Yueqiang<br>
PB22111636 Peng Han<br>
PB22111631 Guo Ze<br>

## Project progress

| Project phase | Time | Project progress | Work schedule |
| :----------------: | :----------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------: |
| Topic selection phase | 3.3 - 3.9 | Study previous topics, discuss the optional topics roughly, and then determine the research direction for each person | Group division of labor: 1. Yin Yiming: Distributed memory management and process management. 2. Guo Ze: AI+OS 3. Peng Han: Virtualization 4. He Yueqiang: Network system |
| Topic selection stage | 3.17 - 3.31 | Determine the topic as GPU memory optimization and complete the research report | Group division of labor: 1. Yin Yiming: Project basis + integration. 2. Guo Ze: Project background 3. Peng Han: Forward-looking analysis 4. He Yueqiang: Related work |
| Topic selection stage | 4.8 - 4.10 | Change the research topic to RAY+large model and conduct urgent research in related directions | Group division of labor: 1. Yin Yiming: Ray. 2. Guo Ze: Deepzero 3. Peng Han: Spark 4. He Yueqiang: vllm |
| Feasibility report | 4.10 - 4.12 | Feasibility report content and division of labor discussion, report scope: Ray + feasibility analysis of large models, related optimization direction comparison: Deepzero, spark, vllm | Division of labor: Yin Yiming: vllm feasibility test Guo Ze: Ray feasibility test Peng Han: spark He Yueqiang: deepzero |
| Midterm report | 4.15 - 4.21 | Main tasks of the meeting: planning PPT content and division of labor, communicating project progress and difficulties. We divided the PPT into What: Ray introduction. Why: why use ray, comparison of deepzero and spark and other technologies, introduction of work related to using ray + large models. How: Future research plans, some arrangements that must be made for options | Division of labor: Speech: Yin Yiming PPT production: Guo Ze: why Peng Han: how He Yueqiang: what Feasibility demonstration: Yin Yiming: vllm test, Guo Ze: ray test |
| Ray+ large model basics | 5.4 - 5.10 | Learn the structure and principles of Ray, build and configure the environment, prepare for its deployment on large models, learn the use of the vllm framework and write test codes | Conduct model training-related learning and framework construction |
| Ray+ large model basics | 5.10 - 5.31 | Complete the single-machine deployment of RAY+ large models, and then implement heterogeneous deployment on three PCs in the group | Conduct model training-related learning and framework construction |
| Ray+ large model basics | 6.1 - 6.12 | Study deepspeed/zero/vllm source code in the gap before the final exam | Conduct model training-related learning |
| Ray+ large model optimization | 6.24 - 6.30 | Try ray+llm optimization (vllm, deepspeed) | Peng Han, Guo Ze: Learn ray-llm official warehouse, try ray+vllm deployment Yin Yiming, He Yueqiang: Learn deepspeed, complete relevant baseline tests (samples/s) |
| Ray+ large model optimization part | 7.1 - 7.6 | Try ray+llm optimization (automatic data distribution), optimization result test (throughput, time) | Daily division of labor is determined by the meeting of the day |