# 项目背景

****

**时代背景：**  随着深度学习模型的发展和应用场景的增多，模型变得越来越大，对计算资源的需求也越来越高，特别是对显存的需求。虽然GPU的性能在不断提升，但显存的发展速度却相对较慢，导致在处理大型模型时出现了显存不足的情况。

<img src="./MARKDOWN.asset/0.png" alt="Alt pic" style="zoom: 33%;" /> 

​	InceptionV4设置batch size为32训练 ImageNet需要 40GB显存空间； 

​	BERT拥有768个隐藏层，在Batch size设 置为64时需要73GB的显存空间；

​	使用ImageNet训练Wide ResNet-152，并 设置Batch size为64需要显存180GB

 <img src="./MARKDOWN.asset/1.png" alt="Alt pic" style="zoom: 33%;" />

​	有数据表明深度学习模型（例如GPT-3）这种模型动不动就上百GB，而一个最好的GPU（A100）目前也才80GB的显存容量。解决显存不足已经是人工智能领域的一个重要挑战。一开始，我们可能会尝试让现有的GPU能够训练更大的模型，这可以通过优化算法、减少模型大小等方式来实现。随着技术的进步，我们还可以探索如何更好地利用有限的资源，例如通过更有效的内存管理、数据压缩等技术来提高训练速度和效率。

**现状与前景：** 

​	从2016年发表在MICRO16（体系结构A会）上的vDNN，借助大得多的CPU内存，把训练当前阶段不需要的数据转移到CPU上，借助CPU上的大内存暂存数据，从而减少对显存的需求。

<img src="./MARKDOWN.asset/2.png" alt="Alt pic" style="zoom: 33%;" />

​	到后来重计算和压缩、量化、剪枝技术的不断发展， GPU显存的优化不断深化发展，有了长足进步。

<img src="./MARKDOWN.asset/3.png" alt="Alt pic" style="zoom: 33%;" />

​	到如今，这个领域已经白热化了，随着工作越做越细致，再提出新的解决思路已经比较困难了。然而，随着人工智能模型的发展，目前的GPU显存需求仍然迫切，针对显存优化，AI+System的未来还是有很长的路要走。

